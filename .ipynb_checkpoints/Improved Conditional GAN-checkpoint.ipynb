{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Generative Adversarial Network (Improved)\n",
    "\n",
    "Conditional Generative Adversarial Nets - [Paper](https://arxiv.org/pdf/1411.1784.pdf)\n",
    "\n",
    "Conditional GANs are constructed by feeding the data we wish to condition on to both the generator and discriminator.\n",
    "\n",
    "$\\min\\limits_G\\max\\limits_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)} [log D(x|y)] + \\mathbb{E}_{z\\sim p_z(z)} [log(1 - D(G(z|y)))]$\n",
    "\n",
    "### Improvements\n",
    "\n",
    "* Wasserstein loss for discriminator and generator models\n",
    "* Linear activation in discriminator output instead of sigmoid\n",
    "* Clip discriminator weights after each update\n",
    "* Label smoothing for real outputs\n",
    "* RMSprop instead of Adam\n",
    "    \n",
    "Note: The following implementation is for continuous variables. For discrete variables, implement an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from mpl_toolkits import mplot3d\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.synthetic import plot_data, get_normal_data\n",
    "\n",
    "X, y = get_normal_data(1000, xy_features=(2,1))\n",
    "plot_data(X, y)\n",
    "\n",
    "n = len(X)\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset \\\n",
    "    .from_tensor_slices((X,y)) \\\n",
    "    .shuffle(n).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "\n",
    "class CGAN:\n",
    "    \"\"\"Generate y conditioned on x.\"\"\"\n",
    "    def __init__(self, x_features, y_features, latent_dim=32, g_hidden=16, d_hidden=16, label_smooth=0.9, d_dropout=0.4, d_clip=0.01):\n",
    "        self.x_features = x_features\n",
    "        self.y_features = y_features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.g_hidden = g_hidden\n",
    "        self.d_hidden = d_hidden\n",
    "        self.label_smooth = label_smooth\n",
    "        self.d_dropout = d_dropout\n",
    "        self.d_clip = d_clip\n",
    "        self.g_optimizer = RMSprop(1e-4)\n",
    "        self.d_optimizer = RMSprop(1e-4)\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.generator.summary()\n",
    "        self.discriminator.summary()\n",
    "    \n",
    "    def build_generator(self):\n",
    "        \"\"\"Generator model consists of a dense layer after each component.\"\"\"\n",
    "        noise = Input(shape=(self.latent_dim,))  # noise\n",
    "        d_noise = Dense(self.g_hidden)(noise)\n",
    "        x = Input(shape=(self.x_features,))  # condition\n",
    "        d_x = Dense(self.g_hidden)(x)\n",
    "        z = Concatenate()([d_noise, d_x])\n",
    "        d_z = Dense(self.g_hidden)(z)\n",
    "        y = Dense(self.y_features)(d_z)\n",
    "        return Model([noise, x], y)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        \"\"\"Discriminator model consists of a dense layer after each component.\"\"\"\n",
    "        x = Input(shape=(self.x_features))  # condition\n",
    "        d_x = Dense(self.d_hidden)(x)\n",
    "        y = Input(shape=(self.y_features))  # y\n",
    "        d_y = Dense(self.d_hidden)(y)\n",
    "        h = Concatenate()([d_x, d_y])\n",
    "        h = Dense(self.d_hidden)(h)\n",
    "        h = Dropout(self.d_dropout)(h)\n",
    "        p = Dense(1)(h)\n",
    "        model = Model([y, x], p)\n",
    "        return model\n",
    "    \n",
    "    def g_loss(self, fake_y):\n",
    "        return -tf.math.reduce_mean(fake_y)\n",
    "    \n",
    "    def d_loss(self, real_y, fake_y):\n",
    "        return -tf.math.reduce_mean(real_y * self.label_smooth) + tf.math.reduce_mean(fake_y)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, X, real_y):\n",
    "        noise = tf.random.normal((X.size, self.latent_dim))\n",
    "        \n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            fake_y = self.generator([noise, X], training=True)\n",
    "            \n",
    "            real_pred = self.discriminator([real_y, X], training=True)\n",
    "            fake_pred = self.discriminator([fake_y, X], training=True)\n",
    "            \n",
    "            g_loss = self.g_loss(fake_pred)\n",
    "            d_loss = self.d_loss(real_pred, fake_pred)\n",
    "            \n",
    "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Clip discriminator weights\n",
    "        for var in self.discriminator.trainable_variables:\n",
    "            var.assign(tf.clip_by_value(var, -self.d_clip, self.d_clip))\n",
    "        \n",
    "        return g_loss, d_loss\n",
    "    \n",
    "    def fit(self, dataset, epochs=300):\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for X_train, y_train in dataset:\n",
    "                g_loss, d_loss = self.train_step(X_train, y_train)\n",
    "                g_losses.append(g_loss)\n",
    "                d_losses.append(d_loss)\n",
    "            if epoch % (epochs // 10) == 0:\n",
    "                print(f\"{epoch} [D loss: {d_loss}] [G loss: {g_loss}]\")\n",
    "        plt.plot(g_losses, label='g_loss')\n",
    "        plt.plot(d_losses, label='d_loss')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "    def sample(self, X):\n",
    "        noise = np.random.normal(0, 1, (X.shape[0], self.latent_dim)).astype(np.float32)\n",
    "        return cgan.generator([noise, X]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=32\n",
    "cgan = CGAN(x_features=X.shape[1],\n",
    "            y_features=y.shape[1],\n",
    "            latent_dim=latent_dim,\n",
    "            batch_size=batch_size)\n",
    "cgan.fit(dataset, epochs=1000)\n",
    "y_hat = cgan.sample(X)\n",
    "\n",
    "plot_data(X, y)\n",
    "plot_data(X, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation.visualisation import Visualisation\n",
    "\n",
    "viz = Visualisation(cgan, X, y)\n",
    "\n",
    "# Plot the density function based on X1 and X2\n",
    "viz.plot_prob(X[:,0], y[:,0], bins=20)\n",
    "viz.plot_prob(X[:,1], y[:,0], bins=20)\n",
    "\n",
    "# Plot probability density function given X_fixed across y_range.\n",
    "# viz.plot_prob_fixed(X_fixed=(20,60), y_range=[0,200], count=200)\n",
    "\n",
    "# Given X, plot a violin of y based on X within a tolerance, to compare the distribution between fitted and actual.\n",
    "# X_tol represents the tolerance to which X data is binned to get the corresponding y.\n",
    "# X_fixed represents the X fed into the model to sample y.\n",
    "viz.plot_prob_violin(X_fixed=(20,60), X_tol=(0.1,0.1))\n",
    "\n",
    "# Given X and y, plot a violin of y based on binned X, to compare between fitted and actual.\n",
    "# For the actual distribution, bin X and plot violin plots for the density of y.\n",
    "# For the fitted distribution, input the mean of binned X and plot violin plots for the density of sampled y.\n",
    "viz.plot_binned_violin(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (grab)",
   "language": "python",
   "name": "grab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
