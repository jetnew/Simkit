{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Generative Adversarial Network (CGAN)\n",
    "\n",
    "Conditional Generative Adversarial Nets - [Paper](https://arxiv.org/pdf/1411.1784.pdf)\n",
    "\n",
    "Conditional GANs are constructed by feeding the data we wish to condition on to both the generator and discriminator.\n",
    "\n",
    "$\\min\\limits_G\\max\\limits_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)} [log D(x|y)] + \\mathbb{E}_{z\\sim p_z(z)} [log(1 - D(G(z|y)))]$\n",
    "\n",
    "<img align=\"center\" src=\"https://user-images.githubusercontent.com/27071473/84100915-6b6a1f80-aa3f-11ea-976b-a2b53e449205.png\" width=40%>\n",
    "\n",
    "Note: The following implementation is for continuous variables. For discrete variables, implement an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from mpl_toolkits import mplot3d\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.synthetic import get_tf_normal_dataset\n",
    "\n",
    "dataset = get_tf_normal_dataset(1000, xy_features=(2,1), plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "\n",
    "class CGAN:\n",
    "    \"\"\"Generate y conditioned on x.\"\"\"\n",
    "    def __init__(self, x_features, y_features, latent_dim, batch_size=32):\n",
    "        self.x_features = x_features\n",
    "        self.y_features = y_features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = BinaryCrossentropy()\n",
    "        self.g_optimizer = Adam(1e-4)\n",
    "        self.d_optimizer = Adam(1e-4)\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.generator.summary()\n",
    "        self.discriminator.summary()\n",
    "        \n",
    "    def build_generator(self):\n",
    "        noise = Input(shape=(self.latent_dim,))  # noise\n",
    "        d_noise = Dense(16)(noise)\n",
    "        x = Input(shape=(self.x_features,))  # condition\n",
    "        d_x = Dense(16)(x)\n",
    "        z = Concatenate()([d_noise, d_x])\n",
    "        d_z = Dense(16)(z)\n",
    "        y = Dense(self.y_features)(d_z)\n",
    "        return Model([noise, x], y)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        x = Input(shape=(self.x_features))  # condition\n",
    "        d_x = Dense(16)(x)\n",
    "        y = Input(shape=(self.y_features))  # y\n",
    "        d_y = Dense(16)(y)\n",
    "        h = Concatenate()([d_x, d_y])\n",
    "        h = Dense(16)(h)\n",
    "        h = Dropout(0.4)(h)\n",
    "        p = Dense(1, activation='sigmoid')(h)\n",
    "        model = Model([y, x], p)\n",
    "        return model\n",
    "    \n",
    "    def g_loss(self, fake_y):\n",
    "        return self.loss(tf.ones_like(fake_y), fake_y)\n",
    "    \n",
    "    def d_loss(self, real_y, fake_y):\n",
    "        return 0.5 * (self.loss(tf.ones_like(real_y), real_y) + self.loss(tf.zeros_like(fake_y), fake_y))\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, X, real_y):\n",
    "        noise = tf.random.normal((self.batch_size, self.latent_dim))\n",
    "        \n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            fake_y = self.generator([noise, X], training=True)\n",
    "            \n",
    "            real_pred = self.discriminator([real_y, X], training=True)\n",
    "            fake_pred = self.discriminator([fake_y, X], training=True)\n",
    "            \n",
    "            g_loss = self.g_loss(fake_pred)\n",
    "            d_loss = self.d_loss(real_pred, fake_pred)\n",
    "            \n",
    "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "        return g_loss, d_loss\n",
    "    \n",
    "    def fit(self, dataset, epochs=1000):\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for X_train, y_train in dataset:\n",
    "                g_loss, d_loss = self.train_step(X_train, y_train)\n",
    "                g_losses.append(g_loss)\n",
    "                d_losses.append(d_loss)\n",
    "            if epoch % (epochs // 10) == 0:\n",
    "                print(f\"{epoch} [D loss: {d_loss}] [G loss: {g_loss}]\")\n",
    "        plt.plot(g_losses, label='g_loss')\n",
    "        plt.plot(d_losses, label='d_loss')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "    def sample(self, X):\n",
    "        noise = np.random.normal(0, 1, (X.shape[0], self.latent_dim)).astype(np.float32)\n",
    "        return cgan.generator([noise, X]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.cgan import CGAN\n",
    "\n",
    "latent_dim=32\n",
    "cgan = CGAN(x_features=2,\n",
    "            y_features=1,\n",
    "            latent_dim=latent_dim,\n",
    "            batch_size=32)\n",
    "cgan.fit(dataset, epochs=200)\n",
    "y_hat = cgan.sample(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y)\n",
    "plot_data(X, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation.visualisation import Visualisation\n",
    "\n",
    "viz = Visualisation(cgan, X, y)\n",
    "\n",
    "# Plot the density function based on X1 and X2\n",
    "viz.plot_prob(X[:,0], y[:,0], bins=20)\n",
    "viz.plot_prob(X[:,1], y[:,0], bins=20)\n",
    "\n",
    "# Plot probability density function given X_fixed across y_range.\n",
    "# viz.plot_prob_fixed(X_fixed=(20,60), y_range=[0,200], count=200)\n",
    "\n",
    "# Given X, plot a violin of y based on X within a tolerance, to compare the distribution between fitted and actual.\n",
    "# X_tol represents the tolerance to which X data is binned to get the corresponding y.\n",
    "# X_fixed represents the X fed into the model to sample y.\n",
    "viz.plot_prob_violin(X_fixed=(20,60), X_tol=(0.1,0.1))\n",
    "\n",
    "# Given X and y, plot a violin of y based on binned X, to compare between fitted and actual.\n",
    "# For the actual distribution, bin X and plot violin plots for the density of y.\n",
    "# For the fitted distribution, input the mean of binned X and plot violin plots for the density of sampled y.\n",
    "viz.plot_binned_violin(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (grab)",
   "language": "python",
   "name": "grab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
