{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Regression\n",
    "\n",
    "* Paper: http://www.stat.rice.edu/~hgsung/thesis.pdf\n",
    "* Code: https://github.com/AlexanderFabisch/gmr/blob/master/gmr/gmm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T08:07:14.994791Z",
     "start_time": "2020-05-29T08:07:14.982845Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "def plot_data(X, y):\n",
    "    \"\"\"Plot 2D or 3D according to X dimension.\"\"\"\n",
    "    if X.shape[1] == 1:\n",
    "        for i in range(y.shape[1]):\n",
    "            plt.plot(X, y[:,i], '.')\n",
    "            plt.show()\n",
    "    if X.shape[1] == 2:\n",
    "        for i in range(y.shape[1]):\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.scatter(X[:,0], X[:,1], y[:,i], c=y[:,i], cmap='viridis')\n",
    "            plt.show()\n",
    "\n",
    "def get_normal_data(n=1000, plot=False, xy_features=(2,1)):\n",
    "    \"\"\"Get normal dataset of x=(n*4,2), y=(n*4,1).\"\"\"\n",
    "    x0 = np.concatenate([\n",
    "        np.random.normal(10, 2, size=n),\n",
    "        np.random.normal(10, 2, size=n),\n",
    "        np.random.normal(20, 2, size=n),\n",
    "        np.random.normal(20, 2, size=n),\n",
    "    ])\n",
    "    x1 = np.concatenate([\n",
    "        np.random.normal(30, 2, size=n),\n",
    "        np.random.normal(40, 2, size=n),\n",
    "        np.random.normal(30, 2, size=n),\n",
    "        np.random.normal(40, 2, size=n),\n",
    "    ])\n",
    "    y = np.concatenate([\n",
    "        np.random.normal(50, 2, size=n),\n",
    "        np.random.normal(60, 2, size=n),\n",
    "        np.random.normal(70, 2, size=n),\n",
    "        np.random.normal(80, 2, size=n),\n",
    "    ])\n",
    "    data = np.stack((x0, x1, y), axis=1).astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    if plot:\n",
    "        ax = plt.axes(projection='3d')\n",
    "        ax.scatter(data[:,0], data[:,1], data[:,2], alpha=0.3, c='r')\n",
    "        plt.show()\n",
    "    \n",
    "    return (data[:,:2], np.expand_dims(data[:,2], axis=1)) \\\n",
    "            if xy_features == (2,1) else \\\n",
    "            (np.expand_dims(data[:,0], axis=1), data[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T08:07:15.583492Z",
     "start_time": "2020-05-29T08:07:15.565540Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 2), (4000, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_normal_data()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T08:06:47.440625Z",
     "start_time": "2020-05-29T08:06:47.431668Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import numbers\n",
    "\n",
    "\n",
    "# Taken from scikit learn to get rid of the dependency\n",
    "\n",
    "def check_random_state(seed):\n",
    "    \"\"\"Turn seed into a np.random.RandomState instance\n",
    "    If seed is None, return the RandomState singleton used by np.random.\n",
    "    If seed is an int, return a new RandomState instance seeded with seed.\n",
    "    If seed is already a RandomState instance, return it.\n",
    "    Otherwise raise ValueError.\n",
    "    \"\"\"\n",
    "    if seed is None or seed is np.random:\n",
    "        return np.random.mtrand._rand\n",
    "    if isinstance(seed, (numbers.Integral, np.integer)):\n",
    "        return np.random.RandomState(seed)\n",
    "    if isinstance(seed, np.random.RandomState):\n",
    "        return seed\n",
    "    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'\n",
    "                     ' instance' % seed)\n",
    "\n",
    "\n",
    "def pinvh(a, cond=None, rcond=None, lower=True):\n",
    "    \"\"\"Compute the (Moore-Penrose) pseudo-inverse of a hermetian matrix.\n",
    "    Calculate a generalized inverse of a symmetric matrix using its\n",
    "    eigenvalue decomposition and including all 'large' eigenvalues.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : array, shape (N, N)\n",
    "        Real symmetric or complex hermetian matrix to be pseudo-inverted\n",
    "    cond, rcond : float or None\n",
    "        Cutoff for 'small' eigenvalues.\n",
    "        Singular values smaller than rcond * largest_eigenvalue are considered\n",
    "        zero.\n",
    "        If None or -1, suitable machine precision is used.\n",
    "    lower : boolean\n",
    "        Whether the pertinent array data is taken from the lower or upper\n",
    "        triangle of a. (Default: lower)\n",
    "    Returns\n",
    "    -------\n",
    "    B : array, shape (N, N)\n",
    "    Raises\n",
    "    ------\n",
    "    LinAlgError\n",
    "        If eigenvalue does not converge\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> a = np.random.randn(9, 6)\n",
    "    >>> a = np.dot(a, a.T)\n",
    "    >>> B = pinvh(a)\n",
    "    >>> np.allclose(a, np.dot(a, np.dot(B, a)))\n",
    "    True\n",
    "    >>> np.allclose(B, np.dot(B, np.dot(a, B)))\n",
    "    True\n",
    "    \"\"\"\n",
    "    a = np.asarray_chkfinite(a)\n",
    "    s, u = linalg.eigh(a, lower=lower)\n",
    "\n",
    "    if rcond is not None:\n",
    "        cond = rcond\n",
    "    if cond in [None, -1]:\n",
    "        t = u.dtype.char.lower()\n",
    "        factor = {'f': 1E3, 'd': 1E6}\n",
    "        cond = factor[t] * np.finfo(t).eps\n",
    "\n",
    "    # unlike svd case, eigh can lead to negative eigenvalues\n",
    "    above_cutoff = (abs(s) > cond * np.max(abs(s)))\n",
    "    psigma_diag = np.zeros_like(s)\n",
    "    psigma_diag[above_cutoff] = 1.0 / s[above_cutoff]\n",
    "\n",
    "    return np.dot(u * psigma_diag, np.conjugate(u).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T08:06:47.886166Z",
     "start_time": "2020-05-29T08:06:47.864225Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "def invert_indices(n_features, indices):\n",
    "    inv = np.ones(n_features, dtype=np.bool)\n",
    "    inv[indices] = False\n",
    "    inv, = np.where(inv)\n",
    "    return inv\n",
    "\n",
    "\n",
    "class MVN(object):\n",
    "    \"\"\"Multivariate normal distribution.\n",
    "    Some utility functions for MVNs. See\n",
    "    http://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "    for more details.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : array, shape (n_features), optional\n",
    "        Mean of the MVN.\n",
    "    covariance : array, shape (n_features, n_features), optional\n",
    "        Covariance of the MVN.\n",
    "    verbose : int, optional (default: 0)\n",
    "        Verbosity level.\n",
    "    random_state : int or RandomState, optional (default: global random state)\n",
    "        If an integer is given, it fixes the seed. Defaults to the global numpy\n",
    "        random number generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=None, covariance=None, verbose=0,\n",
    "                 random_state=None):\n",
    "        self.mean = mean\n",
    "        self.covariance = covariance\n",
    "        self.verbose = verbose\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.norm = None\n",
    "\n",
    "    def _check_initialized(self):\n",
    "        if self.mean is None:\n",
    "            raise ValueError(\"Mean has not been initialized\")\n",
    "        if self.covariance is None:\n",
    "            raise ValueError(\"Covariance has not been initialized\")\n",
    "\n",
    "    def from_samples(self, X, bessels_correction=True):\n",
    "        \"\"\"MLE of the mean and covariance.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Samples from the true function.\n",
    "        Returns\n",
    "        -------\n",
    "        self : MVN\n",
    "            This object.\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        bias = 0 if bessels_correction else 1\n",
    "        self.covariance = np.cov(X, rowvar=0, bias=bias)\n",
    "        self.norm = None\n",
    "        return self\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Sample from multivariate normal distribution.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of samples.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Samples from the MVN.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "        return self.random_state.multivariate_normal(\n",
    "            self.mean, self.covariance, size=(n_samples,))\n",
    "\n",
    "    def to_probability_density(self, X):\n",
    "        \"\"\"Compute probability density.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array, shape (n_samples,)\n",
    "            Probability densities of data.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "\n",
    "        X = np.atleast_2d(X)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        C = self.covariance\n",
    "        try:\n",
    "            L = sp.linalg.cholesky(C, lower=True)\n",
    "        except np.linalg.LinAlgError:\n",
    "            C = self.covariance + 1e-6 * np.eye(n_features)\n",
    "            L = sp.linalg.cholesky(C, lower=True)\n",
    "        D = X - self.mean\n",
    "        cov_sol = sp.linalg.solve_triangular(L, D.T, lower=True).T\n",
    "        if self.norm is None:\n",
    "            self.norm = 0.5 / np.pi ** (0.5 * n_features) / sp.linalg.det(L)\n",
    "\n",
    "        DpD = np.sum(cov_sol ** 2, axis=1)\n",
    "        return self.norm * np.exp(-0.5 * DpD)\n",
    "\n",
    "    def marginalize(self, indices):\n",
    "        \"\"\"Marginalize over everything except the given indices.\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : array, shape (n_new_features,)\n",
    "            Indices of dimensions that we want to keep.\n",
    "        Returns\n",
    "        -------\n",
    "        marginal : MVN\n",
    "            Marginal MVN distribution.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "        return MVN(mean=self.mean[indices],\n",
    "                   covariance=self.covariance[np.ix_(indices, indices)])\n",
    "\n",
    "    def condition(self, indices, x):\n",
    "        \"\"\"Conditional distribution over given indices.\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : array, shape (n_new_features,)\n",
    "            Indices of dimensions that we want to condition.\n",
    "        x : array, shape (n_new_features,)\n",
    "            Values of the features that we know.\n",
    "        Returns\n",
    "        -------\n",
    "        conditional : MVN\n",
    "            Conditional MVN distribution p(Y | X=x).\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "        mean, covariance = self._condition(\n",
    "            invert_indices(self.mean.shape[0], indices), indices, x)\n",
    "        return MVN(mean=mean, covariance=covariance,\n",
    "                   random_state=self.random_state)\n",
    "\n",
    "    def predict(self, indices, X):\n",
    "        \"\"\"Predict means and covariance of posteriors.\n",
    "        Same as condition() but for multiple samples.\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : array, shape (n_features_1,)\n",
    "            Indices of dimensions that we want to condition.\n",
    "        X : array, shape (n_samples, n_features_1)\n",
    "            Values of the features that we know.\n",
    "        Returns\n",
    "        -------\n",
    "        Y : array, shape (n_samples, n_features_2)\n",
    "            Predicted means of missing values.\n",
    "        covariance : array, shape (n_features_2, n_features_2)\n",
    "            Covariance of the predicted features.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "        return self._condition(invert_indices(self.mean.shape[0], indices),\n",
    "                               indices, X)\n",
    "\n",
    "    def _condition(self, i1, i2, X):\n",
    "        cov_12 = self.covariance[np.ix_(i1, i2)]\n",
    "        cov_11 = self.covariance[np.ix_(i1, i1)]\n",
    "        cov_22 = self.covariance[np.ix_(i2, i2)]\n",
    "        prec_22 = pinvh(cov_22)\n",
    "        regression_coeffs = cov_12.dot(prec_22)\n",
    "\n",
    "        mean = self.mean[i1] + regression_coeffs.dot((X - self.mean[i2]).T).T\n",
    "        covariance = cov_11 - regression_coeffs.dot(cov_12.T)\n",
    "        return mean, covariance\n",
    "\n",
    "    def to_ellipse(self, factor=1.0):\n",
    "        \"\"\"Compute error ellipse.\n",
    "        An error ellipse shows equiprobable points.\n",
    "        Parameters\n",
    "        ----------\n",
    "        factor : float\n",
    "            One means standard deviation.\n",
    "        Returns\n",
    "        -------\n",
    "        angle : float\n",
    "            Rotation angle of the ellipse.\n",
    "        width : float\n",
    "            Width of the ellipse.\n",
    "        height : float\n",
    "            Height of the ellipse.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "        vals, vecs = sp.linalg.eigh(self.covariance)\n",
    "        order = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[order], vecs[:, order]\n",
    "        angle = np.arctan2(*vecs[:, 0][::-1])\n",
    "        width, height = factor * np.sqrt(vals)\n",
    "        return angle, width, height\n",
    "\n",
    "\n",
    "def plot_error_ellipse(ax, mvn):\n",
    "    \"\"\"Plot error ellipse of MVN.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : axis\n",
    "        Matplotlib axis.\n",
    "    mvn : MVN\n",
    "        Multivariate normal distribution.\n",
    "    \"\"\"\n",
    "    from matplotlib.patches import Ellipse\n",
    "    for factor in np.linspace(0.5, 4.0, 8):\n",
    "        angle, width, height = mvn.to_ellipse(factor)\n",
    "        ell = Ellipse(xy=mvn.mean, width=width, height=height,\n",
    "                      angle=np.degrees(angle))\n",
    "        ell.set_alpha(0.25)\n",
    "        ax.add_artist(ell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T08:06:48.434923Z",
     "start_time": "2020-05-29T08:06:48.406547Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class GMM(object):\n",
    "    \"\"\"Gaussian Mixture Model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int\n",
    "        Number of MVNs that compose the GMM.\n",
    "    priors : array, shape (n_components,), optional\n",
    "        Weights of the components.\n",
    "    means : array, shape (n_components, n_features), optional\n",
    "        Means of the components.\n",
    "    covariances : array, shape (n_components, n_features, n_features), optional\n",
    "        Covariances of the components.\n",
    "    verbose : int, optional (default: 0)\n",
    "        Verbosity level.\n",
    "    random_state : int or RandomState, optional (default: global random state)\n",
    "        If an integer is given, it fixes the seed. Defaults to the global numpy\n",
    "        random number generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components, priors=None, means=None, covariances=None,\n",
    "                 verbose=0, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.priors = priors\n",
    "        self.means = means\n",
    "        self.covariances = covariances\n",
    "        self.verbose = verbose\n",
    "        self.random_state = check_random_state(random_state)\n",
    "\n",
    "    def _check_initialized(self):\n",
    "        if self.priors is None:\n",
    "            raise ValueError(\"Priors have not been initialized\")\n",
    "        if self.means is None:\n",
    "            raise ValueError(\"Means have not been initialized\")\n",
    "        if self.covariances is None:\n",
    "            raise ValueError(\"Covariances have not been initialized\")\n",
    "\n",
    "    def from_samples(self, X, R_diff=1e-4, n_iter=100):\n",
    "        \"\"\"MLE of the mean and covariance.\n",
    "        Expectation-maximization is used to infer the model parameters. The\n",
    "        objective function is non-convex. Hence, multiple runs can have\n",
    "        different results.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Samples from the true function.\n",
    "        R_diff : float\n",
    "            Minimum allowed difference of responsibilities between successive\n",
    "            EM iterations.\n",
    "        n_iter : int\n",
    "            Maximum number of iterations.\n",
    "        Returns\n",
    "        -------\n",
    "        self : MVN\n",
    "            This object.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if self.priors is None:\n",
    "            self.priors = np.ones(self.n_components,\n",
    "                                  dtype=np.float) / self.n_components\n",
    "\n",
    "        if self.means is None:\n",
    "            # TODO k-means++\n",
    "            indices = self.random_state.choice(\n",
    "                np.arange(n_samples), self.n_components)\n",
    "            self.means = X[indices]\n",
    "\n",
    "        if self.covariances is None:\n",
    "            self.covariances = np.empty((self.n_components, n_features,\n",
    "                                         n_features))\n",
    "            for k in range(self.n_components):\n",
    "                self.covariances[k] = np.eye(n_features)\n",
    "\n",
    "        R = np.zeros((n_samples, self.n_components))\n",
    "        for _ in range(n_iter):\n",
    "            R_prev = R\n",
    "\n",
    "            # Expectation\n",
    "            R = self.to_responsibilities(X)\n",
    "\n",
    "            if np.linalg.norm(R - R_prev) < R_diff:\n",
    "                if self.verbose:\n",
    "                    print(\"EM converged.\")\n",
    "                break\n",
    "\n",
    "            # Maximization\n",
    "            w = R.sum(axis=0) + 10.0 * np.finfo(R.dtype).eps\n",
    "            R_n = R / w\n",
    "            self.priors = w / w.sum()\n",
    "            self.means = R_n.T.dot(X)\n",
    "            for k in range(self.n_components):\n",
    "                Xm = X - self.means[k]\n",
    "                self.covariances[k] = (R_n[:, k, np.newaxis] * Xm).T.dot(Xm)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Sample from Gaussian mixture distribution.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of samples.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            Samples from the GMM.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "\n",
    "        mvn_indices = self.random_state.choice(\n",
    "            self.n_components, size=(n_samples,), p=self.priors)\n",
    "        mvn_indices.sort()\n",
    "        split_indices = np.hstack(\n",
    "            ((0,), np.nonzero(np.diff(mvn_indices))[0] + 1, (n_samples,)))\n",
    "        clusters = np.unique(mvn_indices)\n",
    "        lens = np.diff(split_indices)\n",
    "        samples = np.empty((n_samples, self.means.shape[1]))\n",
    "        for i, (k, n_samples) in enumerate(zip(clusters, lens)):\n",
    "            samples[split_indices[i]:split_indices[i + 1]] = MVN(\n",
    "                mean=self.means[k], covariance=self.covariances[k],\n",
    "                random_state=self.random_state).sample(n_samples=n_samples)\n",
    "        return samples\n",
    "\n",
    "    def to_responsibilities(self, X):\n",
    "        \"\"\"Compute responsibilities of each MVN for each sample.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data.\n",
    "        Returns\n",
    "        -------\n",
    "        R : array, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        R = np.empty((n_samples, self.n_components))\n",
    "        for k in range(self.n_components):\n",
    "            R[:, k] = self.priors[k] * MVN(\n",
    "                mean=self.means[k], covariance=self.covariances[k],\n",
    "                random_state=self.random_state).to_probability_density(X)\n",
    "        R_norm = R.sum(axis=1)[:, np.newaxis]\n",
    "        R_norm[np.where(R_norm == 0.0)] = 1.0\n",
    "        R /= R_norm\n",
    "        return R\n",
    "\n",
    "    def to_probability_density(self, X):\n",
    "        \"\"\"Compute probability density.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array, shape (n_samples,)\n",
    "            Probability densities of data.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "\n",
    "        p = [MVN(mean=self.means[k], covariance=self.covariances[k],\n",
    "                 random_state=self.random_state).to_probability_density(X)\n",
    "             for k in range(self.n_components)]\n",
    "        return np.dot(self.priors, p)\n",
    "\n",
    "    def condition(self, indices, x):\n",
    "        \"\"\"Conditional distribution over given indices.\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : array, shape (n_new_features,)\n",
    "            Indices of dimensions that we want to condition.\n",
    "        x : array, shape (n_new_features,)\n",
    "            Values of the features that we know.\n",
    "        Returns\n",
    "        -------\n",
    "        conditional : GMM\n",
    "            Conditional GMM distribution p(Y | X=x).\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "\n",
    "        n_features = self.means.shape[1] - len(indices)\n",
    "        priors = np.empty(self.n_components)\n",
    "        means = np.empty((self.n_components, n_features))\n",
    "        covariances = np.empty((self.n_components, n_features, n_features))\n",
    "        for k in range(self.n_components):\n",
    "            mvn = MVN(mean=self.means[k], covariance=self.covariances[k],\n",
    "                      random_state=self.random_state)\n",
    "            conditioned = mvn.condition(indices, x)\n",
    "            priors[k] = (self.priors[k] *\n",
    "                         mvn.marginalize(indices).to_probability_density(x))\n",
    "            means[k] = conditioned.mean\n",
    "            covariances[k] = conditioned.covariance\n",
    "        priors /= priors.sum()\n",
    "        return GMM(n_components=self.n_components, priors=priors, means=means,\n",
    "                   covariances=covariances, random_state=self.random_state)\n",
    "\n",
    "    def predict(self, indices, X):\n",
    "        \"\"\"Predict means of posteriors.\n",
    "        Same as condition() but for multiple samples.\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : array, shape (n_features_1,)\n",
    "            Indices of dimensions that we want to condition.\n",
    "        X : array, shape (n_samples, n_features_1)\n",
    "            Values of the features that we know.\n",
    "        Returns\n",
    "        -------\n",
    "        Y : array, shape (n_samples, n_features_2)\n",
    "            Predicted means of missing values.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "\n",
    "        n_samples, n_features_1 = X.shape\n",
    "        n_features_2 = self.means.shape[1] - n_features_1\n",
    "        Y = np.empty((n_samples, n_features_2))\n",
    "        for n in range(n_samples):\n",
    "            conditioned = self.condition(indices, X[n])\n",
    "            Y[n] = conditioned.priors.dot(conditioned.means)\n",
    "        return Y\n",
    "\n",
    "    def to_ellipses(self, factor=1.0):\n",
    "        \"\"\"Compute error ellipses.\n",
    "        An error ellipse shows equiprobable points.\n",
    "        Parameters\n",
    "        ----------\n",
    "        factor : float\n",
    "            One means standard deviation.\n",
    "        Returns\n",
    "        -------\n",
    "        ellipses : array, shape (n_components, 3)\n",
    "            Parameters that describe the error ellipses of all components:\n",
    "            angles, widths and heights.\n",
    "        \"\"\"\n",
    "        self._check_initialized()\n",
    "\n",
    "        res = []\n",
    "        for k in range(self.n_components):\n",
    "            mvn = MVN(mean=self.means[k], covariance=self.covariances[k],\n",
    "                      random_state=self.random_state)\n",
    "            res.append((self.means[k], mvn.to_ellipse(factor)))\n",
    "        return res\n",
    "\n",
    "\n",
    "def plot_error_ellipses(ax, gmm, colors=None):\n",
    "    \"\"\"Plot error ellipses of GMM components.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : axis\n",
    "        Matplotlib axis.\n",
    "    gmm : GMM\n",
    "        Gaussian mixture model.\n",
    "    \"\"\"\n",
    "    from matplotlib.patches import Ellipse\n",
    "    from itertools import cycle\n",
    "    if colors is not None:\n",
    "        colors = cycle(colors)\n",
    "    for factor in np.linspace(0.5, 4.0, 8):\n",
    "        for mean, (angle, width, height) in gmm.to_ellipses(factor):\n",
    "            ell = Ellipse(xy=mean, width=width, height=height,\n",
    "                          angle=np.degrees(angle))\n",
    "            ell.set_alpha(0.25)\n",
    "            if colors is not None:\n",
    "                ell.set_color(next(colors))\n",
    "            ax.add_artist(ell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T10:07:10.492935Z",
     "start_time": "2020-05-29T10:07:10.395169Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GMM at 0x240cc698508>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_normal_data()\n",
    "gmm = GMM(n_components=4)\n",
    "gmm.from_samples(X)\n",
    "gmm.condition(np.array([0]), X[:,0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T08:08:06.030640Z",
     "start_time": "2020-05-29T08:07:59.824983Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:04:30.286385Z",
     "start_time": "2020-05-29T13:04:30.279406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.1   2.08  0.604]\n",
      "[[0.025   0.0075  0.00175]\n",
      " [0.0075  0.007   0.00135]\n",
      " [0.00175 0.00135 0.00043]]\n",
      "[0.025   0.007   0.00043]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[4.0, 2.0, 0.60],\n",
    "              [4.2, 2.1, 0.59],\n",
    "              [3.9, 2.0, 0.58],\n",
    "              [4.3, 2.1, 0.62],\n",
    "              [4.1, 2.2, 0.63]])\n",
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_cov = np.cov(X, rowvar=0)\n",
    "X_diag = np.diag(X_cov)\n",
    "\n",
    "print(X_mean)\n",
    "print(X_cov)\n",
    "print(X_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:04:43.612994Z",
     "start_time": "2020-05-29T13:04:43.604996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([4.05684617, 2.07025646, 0.60430625])>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvn.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (grab)",
   "language": "python",
   "name": "grab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
